<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="deep,learning,Logistic,Regression,">










<meta name="description" content="Logistic Regression with a Neural Network mindsetGeneral Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-catimages. I will build a Logis">
<meta name="keywords" content="deep,learning,Logistic,Regression">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic Regression with a Neural Network mindset">
<meta property="og:url" content="http://www.mashangxue123.com/2018/11/10/Logistic Regression with a Neural Network mindset/index.html">
<meta property="og:site_name" content="DmrfCoder的个人主页">
<meta property="og:description" content="Logistic Regression with a Neural Network mindsetGeneral Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-catimages. I will build a Logis">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fx3dzp5164j314k0ne417.jpg">
<meta property="og:updated_time" content="2018-12-05T14:39:17.856Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic Regression with a Neural Network mindset">
<meta name="twitter:description" content="Logistic Regression with a Neural Network mindsetGeneral Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-catimages. I will build a Logis">
<meta name="twitter:image" content="https://ws3.sinaimg.cn/large/006tNbRwgy1fx3dzp5164j314k0ne417.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.mashangxue123.com/2018/11/10/Logistic Regression with a Neural Network mindset/">





  <title>Logistic Regression with a Neural Network mindset | DmrfCoder的个人主页</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DmrfCoder的个人主页</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.mashangxue123.com/2018/11/10/Logistic Regression with a Neural Network mindset/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DmrfCoder">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DmrfCoder的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Logistic Regression with a Neural Network mindset</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-10T23:51:18+08:00">
                2018-11-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a>Logistic Regression with a Neural Network mindset</h1><h2 id="General-Architecture-of-the-learning-algorithm"><a href="#General-Architecture-of-the-learning-algorithm" class="headerlink" title="General Architecture of the learning algorithm"></a>General Architecture of the learning algorithm</h2><p>It’s time to design a simple algorithm to distinguish cat images from non-cat<br>images.</p>
<p>I will build a Logistic Regression, using a Neural Network mindset. The<br>following Figure explains why <strong>Logistic Regression is actually a very simple<br>Neural Network!</strong></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fx3dzp5164j314k0ne417.jpg" alt="image-20181110233216084"></p>
<p><strong>Mathematical expression of the algorithm</strong> :</p>
<p>For one example Missing superscript or subscript argument x^{(i)} :</p>
<p>(1)  z  (  i  )  =  w  T  x  (  i  )  +  b  z^{(i)} = w^T x^{(i)} + b \tag{1}<br>z  (  i  )  =  w  T  x  (  i  )  +  b  (  1  )</p>
<p>(2)  y  ^  (  i  )  =  a  (  i  )  =  s  i  g  m  o  i  d  (  z  (  i  )  )<br>\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}  y  ^  ​  (  i  )  =  a  (<br>i  )  =  s  i  g  m  o  i  d  (  z  (  i  )  )  (  2  )</p>
<p>(3)  L  (  a  (  i  )  ,  y  (  i  )  )  =  −  y  (  i  )  log  ⁡  (  a  (  i<br>)  )  −  (  1  −  y  (  i  )  )  log  ⁡  (  1  −  a  (  i  )  )<br>\mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} )<br>\log(1-a^{(i)})\tag{3}  L  (  a  (  i  )  ,  y  (  i  )  )  =  −  y  (  i  )<br>lo  g  (  a  (  i  )  )  −  (  1  −  y  (  i  )  )  lo  g  (  1  −  a  (  i  )<br>)  (  3  )</p>
<p>The cost is then computed by summing over all training examples:<br>(4)  J  =  1  m  ∑  i  =  1  m  L  (  a  (  i  )  ,  y  (  i  )  )  J =<br>\frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{4}  J  =  m  1  ​<br>i  =  1  ∑  m  ​  L  (  a  (  i  )  ,  y  (  i  )  )  (  4  )</p>
<p><strong>Key steps</strong> :<br>In this exercise, I will carry out the following steps:</p>
<pre><code>- Initialize the parameters of the model
</code></pre><ul>
<li>Learn the parameters for the model by minimizing the cost </li>
<li>Use the learned parameters to make predictions (on the test set) </li>
<li>Analyse the results and conclude </li>
</ul>
<h2 id="Building-the-parts-of-algorithm"><a href="#Building-the-parts-of-algorithm" class="headerlink" title="Building the parts of algorithm"></a>Building the parts of algorithm</h2><p>The main steps for building a Neural Network are:</p>
<ol>
<li>Define the model structure (such as number of input features) </li>
<li>Initialize the model’s parameters </li>
<li>Loop: <ul>
<li>Calculate current loss (forward propagation) </li>
<li>Calculate current gradient (backward propagation) </li>
<li>Update parameters (gradient descent) </li>
</ul>
</li>
</ol>
<p>You often build 1-3 separately and integrate them into one function we call <code>model()</code> .</p>
<h3 id="Helper-functions"><a href="#Helper-functions" class="headerlink" title="Helper functions"></a>Helper functions</h3><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>Using code from “Python Basics”, implement <code>sigmoid()</code> . As we seen in the<br>figure above, I will compute  s  i  g  m  o  i  d  (  w  T  x  +  b  )  =  1<br>1  +  e  −  (  w  T  x  +  b  )  sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T<br>x + b)}}  s  i  g  m  o  i  d  (  w  T  x  +  b  )  =  1  +  e  −  (  w  T<br>x  +  b  )  1  ​  to make predictions. I will use np.exp().</p>
<pre><code>def sigmoid(z):
   &quot;&quot;&quot;
   Compute the sigmoid of z

   Arguments:
   z -- A scalar or numpy array of any size.

   Return:
   s -- sigmoid(z)
   &quot;&quot;&quot;

   ### START CODE HERE ### (≈ 1 line of code)
   s = 1 / (1 + np.exp(-z))
   ### END CODE HERE ###

   return s
</code></pre><h4 id="initialize-with-zeros"><a href="#initialize-with-zeros" class="headerlink" title="initialize_with_zeros"></a>initialize_with_zeros</h4><pre><code>def initialize_with_zeros(dim):
    &quot;&quot;&quot;
    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.

    Argument:
    dim -- size of the w vector we want (or number of parameters in this case)

    Returns:
    w -- initialized vector of shape (dim, 1)
    b -- initialized scalar (corresponds to the bias)
    &quot;&quot;&quot;

    ### START CODE HERE ### (≈ 1 line of code)
    w = np.zeros(shape=(dim, 1))
    b = 0
    ### END CODE HERE ###

    assert (w.shape == (dim, 1))
    assert (isinstance(b, float) or isinstance(b, int))

    return w, b
</code></pre><h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>Now that our parameters are initialized, we can do the “forward” and<br>“backward” propagation steps for learning the parameters.</p>
<p><strong>Exercise:</strong> Implement a function <code>propagate()</code> that computes the cost<br>function and its gradient.</p>
<p><strong>Hints</strong> :</p>
<p>Forward Propagation:</p>
<ul>
<li>I get X </li>
<li>I compute  A  =  σ  (  w  T  X  +  b  )  =  (  a  (  1  )  ,  a  (  2  )  ,  .  .  .  ,  a  (  m  −  1  )  ,  a  (  m  )  )  A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, …, a^{(m-1)}, a^{(m)})  A  =  σ  (  w  T  X  +  b  )  =  (  a  (  1  )  ,  a  (  2  )  ,  .  .  .  ,  a  (  m  −  1  )  ,  a  (  m  )  ) </li>
<li>I calculate the cost function:  J  =  −  1  m  ∑  i  =  1  m  y  (  i  )  log  ⁡  (  a  (  i  )  )  +  (  1  −  y  (  i  )  )  log  ⁡  (  1  −  a  (  i  )  )  J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})  J  =  −  m  1  ​  ∑  i  =  1  m  ​  y  (  i  )  lo  g  (  a  (  i  )  )  +  (  1  −  y  (  i  )  )  lo  g  (  1  −  a  (  i  )  ) </li>
</ul>
<p>Here are the two formulas I will be using:</p>
<p>(5)  ∂  J  ∂  w  =  1  m  X  (  A  −  Y  )  T  \frac{\partial J}{\partial w} =<br>\frac{1}{m}X(A-Y)^T\tag{5}  ∂  w  ∂  J  ​  =  m  1  ​  X  (  A  −  Y  )  T  (<br>5  )<br>(6)  ∂  J  ∂  b  =  1  m  ∑  i  =  1  m  (  a  (  i  )  −  y  (  i  )  )<br>\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m<br>(a^{(i)}-y^{(i)})\tag{6}  ∂  b  ∂  J  ​  =  m  1  ​  i  =  1  ∑  m  ​  (  a  (<br>i  )  −  y  (  i  )  )  (  6  )</p>
<pre><code>def propagate(w, b, X, Y):
   &quot;&quot;&quot;
   Implement the cost function and its gradient for the propagation explained above

   Arguments:
   w -- weights, a numpy array of size (num_px * num_px * 3, 1)
   b -- bias, a scalar
   X -- data of size (num_px * num_px * 3, number of examples)
   Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)

   Return:
   cost -- negative log-likelihood cost for logistic regression
   dw -- gradient of the loss with respect to w, thus same shape as w
   db -- gradient of the loss with respect to b, thus same shape as b

   Tips:
   - Write your code step by step for the propagation. np.log(), np.dot()
   &quot;&quot;&quot;

   m = X.shape[1]

   # FORWARD PROPAGATION (FROM X TO COST)
   ### START CODE HERE ### (≈ 2 lines of code)
   A = sigmoid(np.dot(w.T, X) + b)  # compute activation
   cost = -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A), axis=1)  # compute cost
   ### END CODE HERE ###

   # BACKWARD PROPAGATION (TO FIND GRAD)
   ### START CODE HERE ### (≈ 2 lines of code)
   dw = (1 / m) * np.dot(X, (A - Y).T)
   db = (1 / m) * np.sum(A - Y)
   ### END CODE HERE ###

   assert (dw.shape == w.shape)
   assert (db.dtype == float)
   cost = np.squeeze(cost)
   assert (cost.shape == ())

   grads = {&quot;dw&quot;: dw,
            &quot;db&quot;: db}

   return grads, cost
</code></pre><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul>
<li>I have initialized our parameters. </li>
<li>We are also able to compute a cost function and its gradient. </li>
<li>Now, I want to update the parameters using gradient descent. </li>
</ul>
<p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn w and<br>b by minimizing the cost function J. For a parameter \theta, the update rule<br>is  θ  =  θ  −  α  d  θ  \theta = \theta - \alpha \text{ } d\theta  θ  =  θ  −<br>α  d  θ  , where  α  \alpha  α  is the learning rate.</p>
<pre><code>def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):
   &quot;&quot;&quot;
   This function optimizes w and b by running a gradient descent algorithm

   Arguments:
   w -- weights, a numpy array of size (num_px * num_px * 3, 1)
   b -- bias, a scalar
   X -- data of shape (num_px * num_px * 3, number of examples)
   Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
   num_iterations -- number of iterations of the optimization loop
   learning_rate -- learning rate of the gradient descent update rule
   print_cost -- True to print the loss every 100 steps

   Returns:
   params -- dictionary containing the weights w and bias b
   grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
   costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.

   Tips:
   You basically need to write down two steps and iterate through them:
       1) Calculate the cost and the gradient for the current parameters. Use propagate().
       2) Update the parameters using gradient descent rule for w and b.
   &quot;&quot;&quot;

   costs = []

   for i in range(num_iterations):

       # Cost and gradient calculation (≈ 1-4 lines of code)
       ### START CODE HERE ### 
       grads, cost = propagate(w, b, X, Y)
       ### END CODE HERE ###

       # Retrieve derivatives from grads
       dw = grads[&quot;dw&quot;]
       db = grads[&quot;db&quot;]

       # update rule (≈ 2 lines of code)
       ### START CODE HERE ###
       w = w - learning_rate * dw
       b = b - learning_rate * db
       ### END CODE HERE ###

       # Record the costs
       if i % 100 == 0:
           costs.append(cost)

       # Print the cost every 100 training iterations
       if print_cost and i % 100 == 0:
           print(&quot;Cost after iteration %i: %f&quot; % (i, cost))

   params = {&quot;w&quot;: w,
             &quot;b&quot;: b}

   grads = {&quot;dw&quot;: dw,
            &quot;db&quot;: db}

   return params, grads, costs
</code></pre><h2 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h2><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are<br>able to use w and b to predict the labels for a dataset X. Implement the <code>predict()</code> function. There are two steps to computing predictions:</p>
<ol>
<li>Calculate  Y  ^  =  A  =  σ  (  w  T  X  +  b  )  \hat{Y} = A = \sigma(w^T X + b)  Y  ^  =  A  =  σ  (  w  T  X  +  b  ) </li>
<li>Convert the entries of a into 0 (if activation  &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_prediction</code> . If you wish, you can use an <code>if</code> / <code>else</code> statement in a <code>for</code> loop (though there is also a way to vectorize this). </li>
</ol>
<pre><code>def predict(w, b, X):
    &apos;&apos;&apos;
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)

    Returns:
    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    &apos;&apos;&apos;

    m = X.shape[1]
    Y_prediction = np.zeros((1, m))
    w = w.reshape(X.shape[0], 1)

    # Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture
    ### START CODE HERE ### (≈ 1 line of code)
    A = sigmoid(np.dot(w.T, X) + b)
    ### END CODE HERE ###

    for i in range(A.shape[1]):

        # Convert probabilities A[0,i] to actual predictions p[0,i]
        ### START CODE HERE ### (≈ 4 lines of code)
        if A[0][i] &lt; 0.5:
            Y_prediction[0][i] = 0
        else:
            Y_prediction[0][i] = 1

        ### END CODE HERE ###

    assert (Y_prediction.shape == (1, m))

    return Y_prediction
</code></pre><h4 id="What-to-remember"><a href="#What-to-remember" class="headerlink" title="What to remember"></a>What to remember</h4><p>We’ve implemented several functions that:</p>
<ul>
<li>Initialize (w,b) </li>
<li>Optimize the loss iteratively to learn parameters (w,b): <ul>
<li>computing the cost and its gradient </li>
<li>updating the parameters using gradient descent </li>
</ul>
</li>
<li>Use the learned (w,b) to predict the labels for a given set of examples </li>
</ul>
<h4 id="Merge-all-functions-into-a-model"><a href="#Merge-all-functions-into-a-model" class="headerlink" title="Merge all functions into a model"></a>Merge all functions into a model</h4><p>You will now see how the overall model is structured by putting together all<br>the building blocks (functions implemented in the previous parts) together, in<br>the right order.</p>
<p><strong>Exercise:</strong> Implement the model function. Use the following notation:</p>
<pre><code>- Y_prediction_test for your predictions on the test set
</code></pre><ul>
<li>Y_prediction_train for your predictions on the train set </li>
<li>w, costs, grads for the outputs of optimize() </li>
</ul>
<pre><code>def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):
   &quot;&quot;&quot;
   Builds the logistic regression model by calling the function you&apos;ve implemented previously

   Arguments:
   X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
   Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
   X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
   Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
   num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
   learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
   print_cost -- Set to true to print the cost every 100 iterations

   Returns:
   d -- dictionary containing information about the model.
   &quot;&quot;&quot;

   ### START CODE HERE ###

   # initialize parameters with zeros (≈ 1 line of code)
   w, b = initialize_with_zeros(X_train.shape[0])

   # Gradient descent (≈ 1 line of code)
   params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations=num_iterations, learning_rate=learning_rate,
                                   print_cost=print_cost)

   # Retrieve parameters w and b from dictionary &quot;parameters&quot;
   w = params[&quot;w&quot;]
   b = params[&quot;b&quot;]

   # Predict test/train set examples (≈ 2 lines of code)
   Y_prediction_test = predict(w, b, X_test)
   Y_prediction_train = predict(w, b, X_train)

   ### END CODE HERE ###

   # Print train/test Errors
   print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
   print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

   d = {&quot;costs&quot;: costs,
        &quot;Y_prediction_test&quot;: Y_prediction_test,
        &quot;Y_prediction_train&quot;: Y_prediction_train,
        &quot;w&quot;: w,
        &quot;b&quot;: b,
        &quot;learning_rate&quot;: learning_rate,
        &quot;num_iterations&quot;: num_iterations}

   return d
</code></pre><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><pre><code>d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)
</code></pre><h3 id="test-our-image"><a href="#test-our-image" class="headerlink" title="test our image"></a>test our image</h3><pre><code>## START CODE HERE ## (PUT YOUR IMAGE NAME) 
my_image = &quot;my_image.jpg&quot;   # change this to the name of your image file 
## END CODE HERE ##

# We preprocess the image to fit your algorithm.
fname = &quot;images/&quot; + my_image
image = np.array(ndimage.imread(fname, flatten=False))
my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T
my_predicted_image = predict(d[&quot;w&quot;], d[&quot;b&quot;], my_image)

plt.imshow(image)
print(&quot;y = &quot; + str(np.squeeze(my_predicted_image)) + &quot;, your algorithm predicts a \&quot;&quot; + classes[int(np.squeeze(my_predicted_image)),].decode(&quot;utf-8&quot;) +  &quot;\&quot; picture.&quot;)
</code></pre>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep/" rel="tag"># deep</a>
          
            <a href="/tags/learning/" rel="tag"># learning</a>
          
            <a href="/tags/Logistic/" rel="tag"># Logistic</a>
          
            <a href="/tags/Regression/" rel="tag"># Regression</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/09/Normalizing rows with Python/" rel="next" title="Normalizing rows with Python">
                <i class="fa fa-chevron-left"></i> Normalizing rows with Python
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/13/汇编总结（2）——IA-32处理器基本功能/" rel="prev" title="汇编总结（2）——IA-32处理器基本功能">
                汇编总结（2）——IA-32处理器基本功能 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">DmrfCoder</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">128</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">94</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic-Regression-with-a-Neural-Network-mindset"><span class="nav-number">1.</span> <span class="nav-text">Logistic Regression with a Neural Network mindset</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#General-Architecture-of-the-learning-algorithm"><span class="nav-number">1.1.</span> <span class="nav-text">General Architecture of the learning algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Building-the-parts-of-algorithm"><span class="nav-number">1.2.</span> <span class="nav-text">Building the parts of algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Helper-functions"><span class="nav-number">1.2.1.</span> <span class="nav-text">Helper functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">sigmoid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#initialize-with-zeros"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">initialize_with_zeros</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward-and-Backward-propagation"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Forward and Backward propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimization"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">Optimization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#predict"><span class="nav-number">1.3.</span> <span class="nav-text">predict</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-to-remember"><span class="nav-number">1.3.0.1.</span> <span class="nav-text">What to remember</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Merge-all-functions-into-a-model"><span class="nav-number">1.3.0.2.</span> <span class="nav-text">Merge all functions into a model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train"><span class="nav-number">1.3.1.</span> <span class="nav-text">train</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#test-our-image"><span class="nav-number">1.3.2.</span> <span class="nav-text">test our image</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DmrfCoder</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
